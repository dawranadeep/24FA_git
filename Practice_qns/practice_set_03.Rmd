---
title: "Some Practice Materials on Regression"
subtitle: ""
author: "Ranadeep Daw"
date: "2024-09-07"
output: 
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = T, eval = T)
```


<br><br><br>



## Question 1

Look at the following 4 residuals-against-fitted plots for $4$ simple linear regression problems.


```{r echo =F}
set.seed(123)
x = rnorm(100)
y1 = 3 + 5 * x + rnorm(100, 0, 1)
y1[50] = y1[50] +25
y2 = 3 + 5 * x^2 + rnorm(100, 0, 1)
y3 = 3 + 2 * seq(-2, 2, , 100) + rnorm(100, 0,  2 + 0.1 +  seq(-2, 2, , 100))
y4 = 3 + 5 * c(-10, x) + rnorm(101, 0, 1)
#y4[1] = 3 * y4[1]

par(mfrow = c(1,2))
plot( x, residuals(lm(y1 ~ x)), pch = 16, col = "red", title(main = "Plot 1"), xlab = "Fitted", ylab = "Residuals")
plot( x, residuals(lm(y2 ~ x)), pch = 16, col = "blue", title(main = "Plot 2"), xlab = "Fitted", ylab = "Residuals")

par(mfrow = c(1,2))
plot( seq(-2, 2, , 100), residuals(lm(y3 ~ seq(-2, 2, , 100))), pch = 16, col = "green", title(main = "Plot 3"), xlab = "Fitted", ylab = "Residuals")
plot(  c(-10, x), residuals(lm(y4 ~ c(-10, x))), pch = 16, col = "violet", title(main = "Plot 4"), xlab = "Fitted", ylab = "Residuals")
```

### <br> A. Which plot(s) has mean fitted residual value = 0?


**Answer:**  All of them. In any linear regression model, the sum of the fitted residuals always equals zero, meaning their mean value is also zero.

### <br> B. Which plot(s) shows an outlier?

**Answer:**  On plot 1, there is one point with a residual value that stands out significantly from the others.

In plot 4, observe that one point has a very different fitted value compared to the rest. However, its residual remains close to the mean value of $0$. Thus, this point doesnâ€™t appear to be an outlier, but it may be an influential point for this regression model.

### <br> C. Which plot is most likely to suggest a non-constant variance (or heteroscadasticity)?

**Answer:** In plot 3, see that the variance of the residuals in increasing with respect to the fitted value. This one is most likely to show a non-constant variance.



### <br> D. Which plot suggests a nonlinear relationship?

**Answer:** Plot 2 clearly shows a non-linearity in the data that we haven't captured in the linear regression model.


### <br> E. How would you update the model in the answer to part D? (answers may vary)

**Answer:** The residual pattern in plot 2 appears quadratic, resembling a  $y = x^2$ shape. So, I will first try adding a quadratic term of the predictor.


### <br> F. Comment on plot 4. What are we seeing here?


**Answer:** As discussed in part B, this point is likely very influential (high leverage), despite not differing much from the others in terms of residual values. It may be worth re-checking this value and considering a different model that excludes such a highly influential point.



## Question 2

```{r echo = F}
# Load necessary packages
library(ggplot2)

# Generate random data for training hours and years of experience
set.seed(123)
training_hours <- runif(28, 50, 200)  # Random training hours between 50 and 200
experience <- 25 + 0.0000005 * training_hours + rnorm(28, 0, 10)  # Random linear relationship with noise

# Plot the data
ggplot(data.frame(training_hours, experience), aes(x = training_hours, y = experience)) +
  geom_point() +
  labs(x = "Training Hours", y = "Years of Experience") +
  theme_minimal()

```

### <br> Professional trainers claim they can estimate how many years a person has worked in a particular sector by looking at the number of hours of professional training they've gone through. The following is a plot of years of experience vs number of training hours.

### A. <br> What's your guess for the correlation coefficient?

a) close to -1
b) close to 0
c) close to 1
d) it's impossible to tell


**Answer:** Just from guessing from the scatterplot, it seems that there is no relationship. Hence, my guess would be that it's a correlation coefficient close to $0$.


### B. If I fit a linear regression model, what is your guess for the model significance $p$ vlue?

a) close to 1
b) close to 0
c) close to 0.05
d) it's impossible to tell

**Answer**: Since there seems to be no relationship, the linear model would not be significant. Therefore, it seems to be a $p$ value > 0.05. However, it's unclear at this point whether the  p-value is closer to 1 or to 0.05. I did not set the options very nicely here.




### C. Would you believe in the claims by the trainers that they can successfully predict the years of experience?

**Answer**: The claim seems to be meaningless from the scatterplot.


### D. Researchers collected data on shoe size and number of vocabulary words known for a group of children and found a strong positive correlation between the two variables. Based on this information, can we conclude that shoe size causes a higher number of vocabulary words?



a) Yes, because the data show a strong correlation between shoe size and vocabulary.
b) Yes, because larger feet help in language development.
c) No, because there is no relationship between the two variables.
d) No, because while they are correlated, it is likely that a third variable (like age) is influencing both shoe size and vocabulary.



**Answer**: Always remember that <span style='color:red'>correlation does not imply causation</span>. The two variables are related, but likely through a third variable, such as age. As age increases, a child's shoe size and vocabulary both tend to increase. Such correlations are known as <span style='color:red'>spurious correlations</span>, and the third variables (like age in this example) are called <span style='color:red'>confounding variables</span>.

### E. A survey found that there is a high positive correlation between the number of critical hospital beds occupied and the mortality rate in hospitals. A health official suggested reducing the number of hospital beds as a way to lower the mortality rate. What is your way of interpreting this?
 

Options:

a) This is perfect because of the pattern from a linear regression.
b) This is not perfect because we should instead use a logistic regression since death is a 0/1 (binary) variable.
c) Correlation does not imply causality
d) There is no link between the two variables.


**Answer**: Again, <span style='color:red'>correlation does not imply causation</span>. A hospital with more beds for critical patients is likely to receive more critical patients, which may lead to higher mortality numbers. Therefore, my interpretation is that the suggestion does not make sense.


<br><br><br>

## Question 3


### <br> In the following questions, what kind of model you will propose? Your options are linear regression, logistic regression, and a polynomial regression model.


### <br> A. A medical researcher is studying the likelihood of patients developing a disease based on their age, weight, and smoking habits. The response is either "Yes" (disease present) or "No" (disease absent).

**Answer:** Logistic regression since the response is yes/no.


### <br> B. A researcher is modeling the relationship between temperature and crop yield. He found that with temperature increasing, yields initially keep increasing up to a point and then start to decline.


**Answer:** "The response variable is continuous. From the description, the response initially increases and then begins to decrease. A linear model would only be appropriate if the data followed a single pattern (either increasing or decreasing), which does not appear to be the case here. So I would suggest a more complicated model here, such as polynomial regression.

### <br> C. A wildlife biologist wants to model the presence of a certain species of beavers in a habitat based on temperature, distance from the road, and humidity.

**Answer:** The data-type is binary, i.e., we are dealing with "present"/"absent" of the species here. So, clearly, it's a logistic regression problem.

### <br> D. A fitness coach wants to predict calories burned during a workout based on time spent exercising. She observes that beyond a certain point, longer exercise sessions lead to diminishing returns in calories burned.

**Answer:** The response variable is continuous, and the description indicates that there is a nonlinear relationship between time spent exercising and calories burned, with diminishing returns after a certain point. Therefore, I would suggest using a polynomial regression model to capture this complexity.



### <br> E. A biologist is studying the effect of temperature (in degrees Celsius) on the rate of enzyme activity (in units per minute). They observe that as the temperature increases, the enzyme activity tends to increase as well, and the rate of increase appears to be fairly uniform across the range of temperatures studied. 

**Answer:** The response variable is continuous, and the relationship between temperature and enzyme activity seems to be linear, as the rate of increase appears uniform. Therefore, a linear regression model would be appropriate for this analysis.



<br><br>

## Question 4

### Match the statements below with the corresponding terms from the list:

| **Statements**                                                                                                | **Options**                                       |
|---------------------------------------------------------------------------------------------------------------|---------------------------------------------------|
| A. A measure that explains the strength and direction of a linear relationship between two continuous variables. | a. Covariance                     |
| B. A regression approach where variables are removed step-by-step based on statistical significance criteria.   | b. Logit transformation                            |
| C. The measure of the joint variability of two random variables.                                                | c.                     Correlation coefficient                      |
| D. A test for normality of the residuals.                    | d. Shapiro-Wilk test                             |
| E. A regression technique used when the relationship between the dependent variable and predictor variables is nonlinear. | e. R$^2$                           |
| F. Test statistic in the output to test the significance of a linear regression model.                          | f. $F$ statistic                                   |
| G. A statistical method used to predict a continuous outcome from one or more predictors.                       | g. Logistic regression                                   |
| H. A measure of how close the predicted values in a simple regression model are to the actual observed values.   | h. Sum of squared errors (SSE)                     |
| I. The sum of the squared differences between observed and predicted values in a linear regression model.        | i. Mean square error ($\widehat{\sigma}^2$)        |
| J. A regression model used to predict an outcome with two possible categories.                                  | j.       Adjusted $R^2$                      |
| K. A value used to compare models with different numbers of predictors, adjusting for the number of predictors.  | k. Homoscedasticity                                      |
| L. A measure used to assess how well a polynomial regression model fits the data.                               | l. Backward elimination                            |
| M. The assumption that the variance of the errors is constant across all levels of the independent variables.    | m. Polynomial regression                               |
| N. A function that transforms a probability value into any number over the real line.                           | n. Spearmanâ€™s rank correlation coefficient         |
| O. A measure used to describe the direction and strength of a monotonic relationship between variables.          | o. Linear Regression model                            |

**Answer:** Note: you might get a slightly different answer.


* A c
* B l
* C a
* D d
* E m
* F f
* G o
* H i
* I h
* J g
* K j
* L e
* M k
* N b
* O n




## Question 5


Someone considered predicting the electricity costs for their household over the course of a year in both India and the USA. A basic plot of the data reveals a smooth nonlinear trend, with electricity costs peaking in India during the hottest months (May -- July) and peaking in the USA during the coldest months (December -- February). To model these patterns, you decide to fit a quadratic regression model for each country (separately), using the month of the year (January = 1, February = 2, ..., December = 12) as the predictor variable:

$\text{Cost} = \beta_0 + \beta_1 * \text{Month} + \beta_2 * \text{Month}^2 + \epsilon$.

Which one is most likely among the following options?

Options: 

a) $\widehat{\beta}_2$ in India > 0; $\widehat{\beta}_2$ in the USA > 0.
b) $\widehat{\beta}_2$ in India < 0; $\widehat{\beta}_2$ in the USA < 0.
c) $\widehat{\beta}_2$ in India > 0; $\widehat{\beta}_2$ in the USA < 0.
d) $\widehat{\beta}_2$ in India < 0; $\widehat{\beta}_2$ in the USA > 0.
e) It is impossible to guess.


**Answer:** Let's plot some hypothetical graph below.

```{r echo=F}
# Simulated data for electricity costs (in arbitrary units)
months <- 1:12

# Simulated costs for India and USA
cost_india <- c(200, 300, 500, 800, 1000, 1200, 1100, 800, 600, 400, 300, 250)/10 # peak in May-July
cost_usa <- c(1100, 1200, 800, 700, 600, 550, 400, 500, 600, 800, 900, 1000)/10 # peak in Dec-Feb

# Fit quadratic regression models
model_india <- lm(cost_india ~ months + I(months^2))
model_usa <- lm(cost_usa ~ months + I(months^2))

# Summary of models
summary(model_india)
summary(model_usa)

# Plotting the data and regression lines
library(ggplot2)

# Create a data frame for plotting
data <- data.frame(
  Month = rep(months, 2),
  Cost = c(cost_india, cost_usa),
  Country = rep(c("India", "USA"), each = 12)
)

# Plot
ggplot(data, aes(x = Month, y = Cost, color = Country)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE) +
  labs(title = "Electricity Costs in India and USA", 
       x = "Month", 
       y = "Electricity Cost") +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  theme_minimal()

```


When presented with a graph like this, it's important to focus on its shape. If the shape resembles a "U," then $\widehat{\beta}_2$ is greater than 0. If it is like **inverted U**, $\widehat{\beta}_2$ is less than 0.

Here, option d) ($\widehat{\beta}_2$ in India < 0; $\widehat{\beta}_2$ in the USA > 0) seems the best.


<br><br><br>

## Question 6

### A. Use the data (named `dugong_data`) on the age and the length of dugongs and build a regression model that you like. Comment on the scatterplot, your model, and how it is performing.  


```{r}
dugong_data = read.table(url("http://www.statsci.org/data/oz/dugongs.txt"), header = T)
```

**Answer**:

First, plot the scatterplot. You will see that a linear model will not be good enough here.

```{r}
Age = dugong_data$Age
Length = dugong_data$Length

# Assuming Age and Length are your vectors and dugong_data is your data frame
plot(Age, Length, pch = 19, main = "Scatterplot and regression models", xlab = "Age", ylab = "Length")
abline(lm(Length ~ Age), col = "green")
```



Look that the linear regression is not good enough. Now, let's fit a polynomial model with degree $2$. See from the model fit that this one performed much better.

```{r}
Age = dugong_data$Age
Length = dugong_data$Length

# Assuming Age and Length are your vectors and dugong_data is your data frame
plot(Age, Length, pch = 19, main = "Scatterplot and regression models", xlab = "Age", ylab = "Length")
abline(lm(Length ~ Age), col = "green")

# Fit a polynomial regression model
mod2 <- lm(Length ~ poly(Age, 2), data = dugong_data)
lines(Age, fitted(mod2), col = "blue", lwd = 2)  # Changed pch to lwd for the line thickness

# Add a legend
legend("bottomright", legend = c("Linear regression fit", "Polynomial regression fit"), 
       col = c("green", "blue"), lty = 1, lwd = 2, pch = NA)  # Set pch to NA for lines


summary(mod2)

```

From the summary table, the model is significant. All the predictor terms are significant. Therefore, I will consider this my final model. However, you are welcome to explore more complex models if you wish.



### B. This is for future statistics classes. Update the model by using a `log` transformation on both the predictor and the response.



**Answer**:  Now it's time to explore how we can leverage concepts from the scatterplot to apply linear regression. We aim to build the following model:


$$ \log(\text{Length}) = $\beta_0 + \beta_1 * \log(\text{Age}) + \epsilon$$



```{r}
log_Age = log(Age)
log_Length = log(Length)


plot(log_Age, log_Length, pch = 16)

abline( lm(log_Length ~ log_Age), col = "red")
```

See how nicely this data has now converted to alinear model now. 



<br><br><br>

## Question 7

#### <br> Ideas for the last question in the exam

* Consider a logistic regression model predicting the likelihood that a frog has an abnormality. 
* The model outputs probabilities between 0 and 1. 
* These can be converted into predicted classes ("Positive"/"1" or "Negative"/"0") based on a threshold.
* Let's take a threshold of 0.5 and suppose that we have the following data:

| **True Status (Actual)** | **Predicted Probability (Model)** | **Predicted Status (Threshold = 0.5)** |
|--------------------------|-----------------------------------|---------------------------------------|
| 1                  | 0.85                              | 1                              |
| 1                  | 0.45                              | 0                              |
| 0                  | 0.10                              | 0                              |
| 1                  | 0.76                              | 1                              |
| 0                  | 0.60                              | 1                              |
| 0                  | 0.20                              | 0                              |
| 1                  | 0.92                              | 1                              |
| 0                  | 0.15                              | 0                              |

### Definitions and Calculations

1. **True Positives (TP)**: Cases where the model predicted "Positive" and the actual status is "Positive".
    - In the table: 3 true positives (rows 1, 4, and 7).

2. **True Negatives (TN)**: Cases where the model predicted "Negative" and the actual status is "Negative".
    - In the table: 3 true negatives (rows 3, 6, and 8).

3. **False Positives (FP)**: Cases where the model predicted "Positive" but the actual status is "Negative".
    - In the table: 1 false positive (row 5).

4. **False Negatives (FN)**: Cases where the model predicted "Negative" but the actual status is "Positive".
    - In the table: 1 false negative (row 2).

### Sensitivity (True Positive Rate)
Sensitivity measures how well the model identifies actual positives (true positives) out of all actual positives.

\[
\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{3}{3 + 1} = 0.75
\]

This means the logistic regression model correctly identifies 75% of the actual positive cases.

### Specificity (True Negative Rate)
Specificity measures how well the model identifies actual negatives (true negatives) out of all actual negatives.

\[
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{3}{3 + 1} = 0.75
\]

This means the model correctly identifies 75% of actual negative cases.

### ROC Curve (Receiver Operating Characteristic)
The ROC curve plots **True Positive Rate (Sensitivity)** against the **False Positive Rate (1 - Specificity)** at different probability thresholds.

In our case, at a threshold of 0.5:
- Sensitivity = 0.75
- False Positive Rate = 1 - Specificity = 0.25

Other thresholds (e.g., 0.3, 0.7) would yield different pairs of Sensitivity and False Positive Rate, and these would be plotted to generate the ROC curve.




### Logistic Regression Classification with Multiple Thresholds

| **True Status (Actual)** | **Predicted Probability (Model)** | **Predicted Status (Threshold = 0.5)** | **Predicted Status (Threshold = 0.3)** | **Predicted Status (Threshold = 0.7)** |
|--------------------------|-----------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|
| 1                        | 0.85                              | 1                                     | 1                                     | 1                                     |
| 1                        | 0.45                              | 0                                     | 1                                     | 0                                     |
| 0                        | 0.10                              | 0                                     | 0                                     | 0                                     |
| 1                        | 0.76                              | 1                                     | 1                                     | 1                                     |
| 0                        | 0.60                              | 1                                     | 1                                     | 0                                     |
| 0                        | 0.20                              | 0                                     | 0                                     | 0                                     |
| 1                        | 0.92                              | 1                                     | 1                                     | 1                                     |
| 0                        | 0.15                              | 0                                     | 0                                     | 0                                     |

#### Sensitivity and Specificity Calculations:

1. **Threshold = 0.5**
   - **True Positives (TP)**: 3 (Rows 1, 4, 7)
   - **True Negatives (TN)**: 3 (Rows 3, 6, 8)
   - **False Positives (FP)**: 1 (Row 5)
   - **False Negatives (FN)**: 1 (Row 2)
   
   **Sensitivity (Recall)** = TP / (TP + FN) = 3 / (3 + 1) = 0.75  
   **Specificity** = TN / (TN + FP) = 3 / (3 + 1) = 0.75

2. **Threshold = 0.3**
   - **True Positives (TP)**: 4 (Rows 1, 2, 4, 7)
   - **True Negatives (TN)**: 3 (Rows 3, 6, 8)
   - **False Positives (FP)**: 1 (Row 5)
   - **False Negatives (FN)**: 0
   
   **Sensitivity** = 4 / (4 + 0) = 1.00  
   **Specificity** = 3 / (3 + 1) = 0.75

3. **Threshold = 0.7**
   - **True Positives (TP)**: 3 (Rows 1, 4, 7)
   - **True Negatives (TN)**: 4 (Rows 3, 5, 6, 8)
   - **False Positives (FP)**: 0
   - **False Negatives (FN)**: 1 (Row 2)
   
   **Sensitivity** = 3 / (3 + 1) = 0.75  
   **Specificity** = 4 / (4 + 0) = 1.00


### AUC (Area Under the Curve)
The AUC quantifies the overall ability of the logistic regression model to distinguish between positive and negative cases across all thresholds.

- **AUC = 1** represents perfect discrimination.
- **AUC = 0.5** represents no better than random guessing.

In this example, based on the ROC curve, the **AUC might be around 0.94**, indicating great discriminative ability.



![](https://raw.githubusercontent.com/dawranadeep/24FA_git/refs/heads/main/Practice_qns/img/AUC.png)


### Logistic Regression Key Concepts:
- **Probability Threshold**: Determines the cutoff for classifying a case as "Positive" or "Negative" (e.g., threshold = 0.5 or something else).
- **Predicted Probability**: Output of logistic regression, representing the likelihood of the event (e.g., disease presence).
- **ROC/AUC**: Useful for assessing how well the logistic regression model performs across different thresholds.


