---
title: "Analysis of Variance"
subtitle: "ANOVA Tables"
author: "Ranadeep Daw"
date: "`r Sys.Date()`"
output:
  revealjs::revealjs_presentation:
    theme: "solar"
    center: true
    widescreen: true
    self_contained: true
    width: 1600
    height: 900
    df_print: kable
    reveal_options:
      slideNumber: 'c/t'
      previewLinks: true
---
  
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```







## Analysis of Variance


* You have already seen one example of analysis of variance.
* Recall the linear  regression problem:


<span style = "color:#0CAF59">$$y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_p X_{pi} + \epsilon_i$$</span>


* We want to explain the total sum of squares (SST) = $\sum_{i=1}^n (y_i - \bar{y})^2$ in the response variable $y$.



## Analysis of Variance

* We use the linear regression model to predict as:

<span style = "color:#0CAF59">$$ \hat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 X_{1i} + \widehat{\beta}_2 X_{2i} + \ldots + \widehat{\beta}_p X_{pi} $$</span>

* We defined two quantities:
  - Sum of squares of error (SSE) = $\sum_{i=1}^n (y_i - \hat{y}_i)^2$
  - Regression sum of squares (RSS) = $\sum_{i=1}^n (\hat{y}_i - \bar{y})^2$.

* **RSS** is what we can explain through the model.
* **SSE** is what we can not explain.



## Model Significance Test

* If there are $n$ subjects and $p$ predictors in the regression model, SSE has a degrees of freedom $n-p-1$.
* Given $n$ subjects, SST has a degrees of freedom $n-1$.
* The remaining $p$ degrees of freedom is correspoding to RSS.
* We discussed the model significance test: $H_0: \beta_1 = \ldots = \beta_p = 0$; $H_A$: At least one coefficient is not $0.$


## Model Significance Test (cont.)


* We use the $F$ statistic is given by:

<span style = "color:#0CAF59">$$\text{F} = \frac{\frac{\text{RSS}}{p}}{\frac{\text{SSE}}{n-p-1}} $$</span>

* Under $H_0$, F is distributed as a $F$ distribution with degrees of freedoms $p$ and $n-p-1$.

* We reject $H_0$ at level $\alpha$ if F > $F_{\alpha; p, n-p-1}$ (or if the $p$ value < $\alpha).$



## Example 


* As an example, let's again use the $\texttt{mtcars}$ data using the $\texttt{mpg}$ as the dependent variable, and $\texttt{disp}$ as the independent variable.


```{r}
data("mtcars")
mpg_model = lm( mpg ~ disp, data = mtcars )
mpg_model
```



## Example 


```{r}
summary(mpg_model)
```


## ANOVA Table

* In this lecture, we will print the details of the $F$ test in the ANOVA(**AN**alysis **O**f **VA**riance) table and will be of the following format:


| Source        | Df           | Sum Sq        | Mean Sq      | F value      |
|---------------|--------------|---------------|--------------|--------------|
| Predictor 1   | *$df_1$*     | *SS1*         | *MS1* = $\frac{SS1}{df_1}$        | *F1* = $\frac{MS1}{MSE}$         |
| Predictor 2   | *$df_2$*     | *SS2*         | *MS2* = $\frac{SS2}{df_2}$       | *F2*  = $\frac{MS2}{MSE}$       |
| $\vdots$      | $\vdots$     | $\vdots$      | $\vdots$     | $\vdots$     |
| Residuals     | *$df_E$*     | *SSE*      | *MSE* =   $\frac{SSE}{df_E}$   |              |



## ANOVA Table Example in $\texttt{R}$


```{r}
summary.aov(mpg_model)
```

* Also shows you the $p$ value of the ANOVA table.
* Whenever $p$ value < $\alpha$, the corresponding predictor is **significant**.
* This also shows that in the regression model, the predictor variable has a significance in predicting the response. It's not meaningless or removable from the model.





## Categorical Variables

* Before the next part, we need to define <span style = "color:red">categorical variables</span> -- a variable that can only realize a few categories.
* You already saw an example of this -- binary resonse variable:
  - A variable that can realize only two categories - Yes/No; 1/0; Head/Tail.
  
## Categorical Variables (Cont.)


* To generalize further, a categorical variable can observe $k$ different categories: Level $1$, ..., level $k$.
  - **Example 1**: Customer satisfaction levels taking **$5$** possible categories --  
  <span style="color:#FF0000">1. Very dissatisfied</span>,  
  <span style="color:#FF7F00">2. Dissatisfied</span>,  
  <span style="color:#7FFF00">3. Neutral</span>,  
  <span style="color:#008000">4. Satisfied</span>,  
  <span style="color:blue">5. Very satisfied</span>.
  - **Example 2**: A multiple choice question with possible answers Yes/No/Maybe.



  


## ANOVA for Categorical Predictors

* ANOVA tables become more interesting with categorical predictor variables. 
* One may be interested in knowing how different categories of a predictor can affect the response differently. 
* For example, one may divide a set of patients into two groups -- treatment and placebo. We want to know if the treatment works better than the placebo.
* In this case, we typically use the terminology *treatment* to denote the levels of the predictor variables.





## Example: `sleep` data

* Example: In $\texttt{R}$, the `sleep` data has three variables -- `extra` as the response variable ($y$), and `group` as the predictor variable ($x$). `ID` variable are patient IDs.
* The `extra` variable shows the effect of two drugs on increase in hours of sleep.
* The `group` variable is categorical and has two levels -  `1` and `2`.


## Example: `sleep` data


```{r}
data("sleep")
sleep
```


## Example: `sleep` data

* Here, the question of interest is that if there is a difference in the effect of the two drugs on the response: `extra`.
* Careful! We want to check the **difference** only, not the absolute effect of a drug on the response.
* We can now build a regression model with `extra` as response and `group` as predictor first.
* Then, we  can build the ANOVA table to formally test our question of interest.

## ANOVA Table for Categorical Data

* Define $\mu_k$: the mean effect of the predictor when at level $k$.
* So in this example, $\mu_1$ is the mean effect of drug 1 on the patients on the response variable `extra`.
* $\mu_2$ is the mean effect of drug 2.
* Here, we will be interested in fitting the regression model:

$$y_{ij} = \mu_i + \epsilon_{ij}$$

\noindent where $i= 1,2$; $j = 1, \ldots, n_i$.


## ANOVA F Test Hypothesis

* If the two drugs (1 and 2) have the similar effect on the response variable, we can simplify the above model as:
$$y_{ij} = \mu + \epsilon_{ij}$$

* When a predictor at all it's different levels has the exact same effect on the response variable, there is no significance of the predictor in the regression model.

* Therefore, in this regression model, we would be interested in testing the significance of the predictor `group` by testing the following hypothesis: <span style = "color:#0CAF59">$H_0: \mu_1 = \mu_2$</span>, <span style="color:#FF0000">$H_A: \mu_1 \neq \mu_2.$</span>



## Example with `sleep` data

* Let's try to visualize first if the `group` variable has any effect on the response variable `extra`.

* Plot the predictor level-wise boxplot of the response variable to see how the response variable behaves against the predictor variables different levels, or against different treatment effects.


## Example with `sleep` data

```{r out.width=500}
boxplot(extra ~ group, data = sleep,
        main = "Effect of Drug Type on Extra Sleep",
        xlab = "Drug Type (Group)", ylab = "Increase in Sleep (hours)",
        col = c("skyblue", "salmon"), names = c("Drug 1", "Drug 2"))
```





## Example with `sleep` data


* Look at the `boxplot`. The overall pattern is that the drug 2 improves the sleep hour (`extra` or $y$) better than drug 1.
* But, can we prove that, <span style="color:#FF0000">statistically</span>, the predictor `group` has a significant effect on the response `extra`? (i.e., is $\mu_1 \neq \mu_2$?)
* Let's do the formal test in $\texttt{R}$ : $H_0: \mu_1 = \mu_2, H_A: \mu_1 \neq \mu_2.$</span>


## Example with `sleep` data

```{r}
data("sleep")
sleep_model = lm( extra ~ group, data = sleep)
summary.aov(sleep_model)
```


## Example with `sleep` data

* Look at the $p$ value.
* Only when a $p$ value is less than our typical significance level of $0.05$, the predictor becomes significant.
* Otherwise, can't reject $H_0$: $\mu_1 = \mu_2$.
* Hence, here, the predictor is not statistically significant.

## Check with Regression Problem

* Look at the summary of the regression model:

```{r}
summary(sleep_model)
```

* See that the model is not significant, i.e., $p$ value > $\alpha$. 



## Generalized ANOVA with Categorical Data

* Now, it's time to extend the theory for general predictor variables with $k$ levels.
* Consider a regression problem with response $y$ and a predictor variable with $k$ levels.
* Consider that the $k$-th level has an effect $\mu_k$ on the response.
* Then, the regression model can be rewritten as:

$$y_{ij} = \mu_i + \epsilon_{ij}$$

\noindent where $i=1, 2, \ldots, k$; $j= 1,\ldots, n_i$.


 
 
## ANOVA with Categorical Data

* First define: $\bar{y}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} y_{ij}, \bar{y} = \frac{1}{n}\sum_{1}^{n_i}\sum_{j=1}^{n_i} y_{ij}$.

* Define the following:

  - **Total Sum of Squares**: <span style = "color:#0CAF59">$\text{SST} = \sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2.$</span>

  - **Regression Sum of Squares**: <span style = "color:#0CAF59">$\text{RSS} = \sum_{i=1}^k n_i (\bar{y}_i - \bar{y})^2.$</span>

  - **Sum of Squared Errors**: <span style = "color:#0CAF59">$\text{SSE} = \sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2.$</span>

* Note: <span style = "color:red">TSS = RSS + SSE.</span>



##  ANOVA: Degrees of freedoms

* If the total number of observations are $n = n_1 + \ldots + n_k$, the total sum of squares has a degrees of freedom $df_T = n-1$.
* If the predictor variable has $k$ levels, the degrees of freedom is $df_{R} = k-1$.
* The remaining degrees of freedom is given by $n-k$, which is the degrees of freedom for regression, or also known as degrees of freedom for treatment.



##  ANOVA: F test

* $H_0$: $\mu_1 = \ldots = \mu_k$; $H_A$: At least one effect is different.
<br><br>

* F statistic = $\frac{\frac{RSS}{k-1}}{\frac{SSE}{n-k}}.$
* Reject $H_0$ if F > $F_{\alpha; k-1, n-k}$ or if the associated $p$ value < $\alpha$.


## Example 2: `InsectSprays` data in `R`

* The `InsectSprays` data contains $n=72$ observations of insects in agricultural experimental units treated with different insecticides.
* It has two variables -- response variable (or $y$) is the variable `count`, and the predictor variable (or $x$) is named as `spray`.
* The predictor variable `spray` has $k=6$ levels- A, B, C, D, E, F.
* So, $df_T$ = 72-1 = 71, $df_R$ = 6-1 = 5, $df_E$ = $72-6$ = 66.


## Example 2: `InsectSprays` data 

* First, visualize the effect of `spray` on `count`.

```{r out.width=490}
data("InsectSprays") # Don't do this step in Homework
boxplot(count ~ spray, data = InsectSprays,
        xlab = "Type of spray", ylab = "Insect count")
```


## Example 2: `InsectSprays` data 

* Let's build the ANOVA table in `R`.


```{r}
spray_model = lm( count ~ spray, data = InsectSprays )
summary.aov(spray_model)
```



## Example 2: `InsectSprays` data 

* Now, let's formally write the ANOVA F test for this model.
* Note, here we have $6$ levels of the treatment (or predictor variable `spray`) -- $\mu_1, \mu_2, \ldots, \mu_6$.
* SSE (Sum of square of errors) = 1015, with degrees of freedom 66.
* RSS (Regression sum of square) = 2669, with degrees of freedom 5.
* F = (2669/5)/(1015/66) = 34.71.
* Associated $p$ value < $\alpha = 0.05$. Hence, reject $H_0$ of ANOVA F test that $\mu_1 = \ldots = \mu_6$.



## Plotting

* You can similarly do the residual-against-fitted plot and QQplot just like a regression model.


```{r out.width=450}
plot(  spray_model, which = 1 )
```


## Plotting 


```{r out.width=600}
plot(  spray_model, which = 2 )
```




## Example 3: `PlantGrowth` data 

* Another inbuilt `R` data. It has a response variable `weight` and a categorical predictor `group` with possible levels `ctrl`, `trt1`, and `trt2`.
* The data has $n=30$ observations.

* Let's build the ANOVA table:


## Example 3: `PlantGrowth` data 

```{r}
data("PlantGrowth")
PlantGrowth
```


## Example 3: `PlantGrowth` data 

```{r}
plant_model = lm(weight ~ group, data = PlantGrowth)
summary.aov(plant_model)
```


## Example 3: `PlantGrowth` data

* See that since the predictor `group` has k=3 levels, the degrees of freedom = 2.
* Total degrees of freedom = 30-1 = 29.
* F = (3.7663 /2)/(10.4921/27) = 4.846.
* Reject $H_0$ that $\mu_1 = \mu_2 = \mu_3$ since $p$ value < $0.05$.



## Example 3: `PlantGrowth` data

Also, visualize the effect of the predictor `group` on response `weight`.


```{r out.width=500}
boxplot(weight ~ group, data = PlantGrowth)
```


## Tukey's Multiple Comparison

* Once your $H_0$ is rejected, you only know that at least one of your treatment effects ($\mu_i$) are different from others.
* However, you don't know exactly which one is different, or how they behave in comparison to each other.
* The  Tukey HSD (Honestly Significant Difference) helps you compare the differences between the treatment effects $\mu_i - \mu_j$.
* The $\texttt{R}$ code is `TukeyHSD(aov(<model_name>))`.


##  Tukey's Multiple Comparison: Example

* In the last example of `PlantGrowth` data, the TukeyHSD() output will show the pairwise comparisons between the groups (ctrl vs. trt1, ctrl vs. trt2, and trt1 vs. trt2).
* It will show the difference in means, confidence intervals, and adjusted p-values.

```{r}
TukeyHSD(aov(plant_model))
```

## Tukey's Multiple Comparison: Example

* **How to interpret the output**?
  - The p-value for trt2 vs. ctrl is significant at level $0.05$, since the associated  p value is smaller than 0.05. This suggests a difference in means between  trt2 and ctrl.
  - The comparisons for trt1 vs. ctrl and trt1 vs. trt2 are not statistically significant, meaning no **statistically significant** difference was found for those pairs.



## Pairwise t test

* Another alternate idea is to perform a pairwise t tests between a pair of effects of the treatments.
* So for example, in this data, we will perform multiple t tests for the difference between trt1 vs trt2, trt1 vs ctrl., trt2 vs. ctrl.
* Due to the multiplicity of the number of t tests, here one needs to adjust the $p$ values of the tests because it increases the likelihood of finding at least one statistically significant result purely by chance.



## Pairwise t test 

* There are multiple ways to address this:
  - "none": No adjustment.
  - "bonferroni": Bonferroni correction. Divides the significance level by the number of tests.
  - "holm": Holm's method. A stepwise approach to  sequentially adjusting p-values for comparisons.
  - "BH": Benjamini-Hochberg correction.
  - "fdr": False Discovery Rate.


## Example: pairwise t test with no correction

```{r}
pairwise.t.test(PlantGrowth$weight, PlantGrowth$group,
                p.adjust.method = "none")
```


## Example: pairwise t test with Bonferroni's correction

```{r}
pairwise.t.test(PlantGrowth$weight, PlantGrowth$group,
                p.adjust.method = "bonferroni")
```


## Example: pairwise t test with Holm's correction

```{r}
pairwise.t.test(PlantGrowth$weight, PlantGrowth$group,
                p.adjust.method = "holm")
```


## Summary

* You will typically see an increase in the $p$ values when you use an adjustment.
* Different adjustment methods will lead to different increment in the p values, resulting in different type 1 and type 2 errors of the test.
* In our example, so far the results have been consistent at level $0.05$, i.e., only the `trt1 - trt2` has shown significant difference.
* There are other methods, such as Fisher's LSD test, Dunnett’s test, etc., that will also allow you to similarly check the pairwise differences.





## Two-Way ANOVA

* Time to delve deeper.
* Up until now, we've worked with one response variable, \( y \), and a single categorical predictor variable, \( x \), with levels \( 1, \ldots, k \).
* Now, we’re expanding to a case with two predictor variables, \( A \) and \( B \).
* Predictor \( A \) has \( a \) levels, labeled \( \alpha_1, \ldots, \alpha_a \).
* Predictor \( B \) has \( b \) levels, labeled \( \beta_1, \ldots, \beta_b \).


## Two-way ANOVA: Example 1

* Suppose you are studying the effects of different fertilizers and sunlight levels on plant growth.
  - **Response Variable:** Plant growth (height or weight).
  - **Predictor A:** Consider 3 types of fertilizers - fertilizer 1,2, and 3.
  - **Predictor B:** Sunlight level with 2 levels — full sunlight and partial sunlight.
* A two-way ANOVA would help determine if plant growth differs significantly across fertilizer types, sunlight levels, and if there's an <span style = "color:0CAF59">interaction effect</span> between the type of fertilizer and sunlight exposure.




## Interaction-free (or Additive) Model 

* First, consider the simpler model, without the interaction term.
* This is also called the additive model/ main effects model.
* The regression model looks like the following:

$$y_{ijk} = \alpha_i + \beta_j + \epsilon_{ijk}$$

  - where $y_{ijk}$ is the $k$-th observation that is receiving the $i$-th level of predictor A (or $\alpha_i$), $j$-th level of predictor B (or $\beta_j$)
  - $\epsilon_{ijk}$ error term of the model.
  

## ANOVA Table format


| Source        | Df           | Sum Sq        | Mean Sq      | F value      |
|---------------|--------------|---------------|--------------|--------------|
| Predictor A   | a - 1        | *SSA*         | *MSA* = $\frac{SSA}{a-1}$        | $F_A = \frac{MSA}{MSE}$         |
| Predictor B   | b - 1        | *SSB*         | *MSB* = $\frac{SSB}{b-1}$       | $F_B = \frac{MSB}{MSE}$       |
| Residuals     | (n-1) - (a-1) - (b-1) = n-a-b+1     | *SSE*      | *MSE* =   $\frac{SSE}{n-a-b+1}$   |              |



## F Test  For Predictor A

  * $H_0$: Predictor A has no significance, or alternatively, $\alpha_1 = \alpha_2 = \ldots = \alpha_a$. 
  * $H_A$: At least one level of A has a different effect from the others.
  * Reject $H_0$ if $F_A > F_{\alpha; a-1, n-a-b+1}$ or if the corresponding $p$ value < $\alpha$.
  
## F Test  For Predictor B

  * $H_0$: Predictor B has no significance, or alternatively, $\beta_1 = \beta_2 = \ldots = \beta_b$. 
  * $H_A$: At least one level of B has a different effect from the others.
  * Reject $H_0$ if $F_B > F_{\alpha; b-1, n-a-b+1}$ or if the corresponding $p$ value < $\alpha$.
  
  

## Example: `warpbreaks` data

* This data set is on the number of warp breaks per loom. It has 54 observations on 3 variables.
  -	`breaks`:		The number of breaks
  -	`wool`:	Type of wool (`A` or `B`)
  -	`tension`:	The level of tension (`L`, `M`, `H`)


* Let's see the boxplot of effects, build the regression model, and the ANOVA table.

## Example: `warpbreaks` data

```{r out.width=550}
boxplot(breaks ~ wool + tension, data = warpbreaks,
        col = c("#66C2A5", "#FC8D62"),
        xlab = "Wool Type and Tension Level", ylab = "Number of Breaks",
        names = c("A-Low", "B-Low", "A-Medium", "B-Medium", "A-High", "B-High")) 
```

## Example: `warpbreaks` data

```{r}
data("warpbreaks") # No need of this line in your homework
warp_model = lm(breaks ~ wool + tension, data = warpbreaks)
summary.aov(warp_model)
```


## Test for Variable `wool`

* $H_0$: Effect of wool type A = Effect of wool type B.
* $H_A$: Effect of wool type A $\neq$ Effect of wool type B.
* Check the $p$ value (0.07) in the ANOVA table. If we are testing at level $\alpha = 0.05$, we can not reject $H_0$.
* Therefore, in this regression model, `wool` does not have a significant contribution (or effect of different levels of `wool` are not statistically different).


## Test for Variable `tension`

* $H_0$:  The effect of low, medium, and high tension levels are equal.
* $H_A$: At least one tension level has a different effect from the others.
* Check the $p$ value (0.001) in the ANOVA table. We  can reject $H_0$.
* Therefore, in this regression model, `tension` has a significant effect in the model, suggesting that at least one level of `tension` differs significantly from the others in terms of affecting the response.



## Postdoc Testing/ Multiple Comparison

* Let's try the Tukey's HSD test first to see which `tension` levels are statistically different.

```{r}
TukeyHSD( aov(warp_model)  )
```

## Tukey's Test

* See that the 95% confidence interval for `wool` contains 0. Alternatively, the $p$ value is high (> 0.05). Hence, `wool` is insignificant, or no difference between the 2 levels of `wool`.
* For `tension`, the 95% CI of M-L (medium - low) and H-L are always negative. Hence, these two levels are statistically different.
* You can also look at the $p$ values and figure out which one is different. Remember, $p$ < $\alpha$ implies statistically different from $0$.



## Interaction Model

* In usual real-world cases, the combined effect of two predictors A and B is not simply additive, but depends on the specific levels of each factor. 
* The interaction model helps us to quantify these dependency effects.

* Example: Consider a study on the effects of a new drug and diet-type on blood pressure. Here, 
  - Predictor A: **Medication** with two levels — "drug" and "placebo".
  - Predictor B: **Diet**  with two levels — "low-sodium" and "regular".


## Interaction Model

* **If there is no interaction:** We would expect that each predictor has an additive, independent effect on blood pressure. 
  - For instance, if the medication reduces blood pressure by 10 units and the low-sodium diet reduces it by 5 units, using both might result in a reduction of 15 units.


## Interaction Model

* **If there is an interaction:** The effect of the medication could depend on the type of diet. For example:
  - With a regular diet, the medication might reduce blood pressure by 10 units.
  - However, with a low-sodium diet, the medication could work much better and reduce blood pressure by 20 units instead of the expected 15 units.





## Regression Model with Interaction


* The regression model with interaction looks like the following:

$$y_{ijk} = \alpha_i + \beta_j + (\alpha \beta)_{ij}  + \epsilon_{ijk}$$

* Here  $y_{ijk}$ is the  k-th observation that is receiving the i-th level of predictor A (or $\alpha_i$), j-th level of predictor B (or $\beta_j$), and ($\alpha \beta)_{ij} represents the interaction effect between levels of A and B.

* $\epsilon_{ijk}$  is the error term of the model.




## ANOVA Table format


| Source        | Df           | Sum Sq        | Mean Sq      | F value      |
|---------------|--------------|---------------|--------------|--------------|
| Predictor A   | a - 1        | *SSA*         | *MSA* = $\frac{SSA}{a-1}$        | $F_A = \frac{MSA}{MSE}$         |
| Predictor B   | b - 1        | *SSB*         | *MSB* = $\frac{SSB}{b-1}$       | $F_B = \frac{MSB}{MSE}$       |
| Interaction     | (a-1)(b-1)     | *SSAB*      | *MSAB* =   $\frac{SSAB}{(a-1)(b-1)}$   |         $F_{AB} = \frac{MSAB}{MSE}$      |
| Residuals     | n- a b     | *SSE*      | *MSE* =   $\frac{SSE}{n- ab}$   |              |



## F test for Interaction

* You can do the main effects test (for A and B) similarly.
* To test the interaction, we use the following:
  - $H_0$: No interaction effect or $(\alpha \beta)_{ij}$ = 0 for all i and j.
  - $H_A$: At least one of them is non-zero or there is some interaction effect.
* Reject $H_0$ if F > $F_{\alpha; (a-1)(b-1), n -ab}$ or if the corresponding p value < $\alpha$.



## Example: `warpbreaks` data with interaction term




```{r}
data("warpbreaks") # No need of this line in your homework
warp_model = lm(breaks ~ wool + tension + wool:tension , data = warpbreaks)
summary.aov(warp_model)
```


## F tests

* **Is the interaction term significant**?
  - Yes since the $p$ value is 0.02. This means that the two predictors `wool` and `tension` are interacting based on which level which predictor is at.

* **Is the variable `wool` significant?**
  - No since $p$ value (0.007) < $0.05$.
  
  
* **Is the variable `tension` significant?**
  - Yes since $p$ value > typical level of $0.05$.
  
  

## Tukey's Test

```{r}
TukeyHSD( aov(warp_model) )
```




## Summary

* In ANOVA, the primary goal is to test the impact of various levels of a categorical predictor on a response variable.
* We aim to identify differences in effects across these levels. Differnt effects means significant predictor.
* The F-test begins with the hypothesis that there is no difference in effects among the levels.
* If the F-test indicates a significant result, post hoc tests can be performed to conduct pairwise comparisons.
* In two-way ANOVA models, interaction terms are often essential because the effect of one factor may vary across the levels of another factor.