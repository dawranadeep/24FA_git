---
title: "Two-sample $t$ Tests"
subtitle: "Independent and Dependent Cases"
author: "Ranadeep Daw"
date: "`r Sys.Date()`"
output:
  slidy_presentation:
    theme: flatly
    center: true
    widescreen: true
    width: 1600
    height: 900
    df_print: kable
    reveal_options:
      slideNumber: true
      previewLinks: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```


## Acknowledgement

Examples are from Biostatistical Analysis by Jerrold H. Zar, Prentice Hall, Fifth Edition.


## Practice

Following are the weight change of 17 horses given an antibiotic for two weeks (in kg): <br> <br>
<center> <span style = "color:red">2.0, 1.1, 4.4, -3.1, -1.3, 3.9, 3.2, -1.6, 3.5, 1.2, 2.5, 2.3, 1.9, 1.8, 2.9, -0.3, -2.4 </span></center> <br><br>
Assume normality and let's test for the true population mean $\mu$ of this sample.

<br>

  1. Test for $H_0: \mu = 0$ vs $H_A: \mu \neq 0$. Take $\alpha = 0.05$.
  2. Confidence interval for $\mu$. Take $\alpha = 0.10$.
  3. Test for $\mu \geq 0.5$ at $\alpha = 0.10$. What is the one-sided confidence interval?
  4. Test for $\mu \leq 2$ at $\alpha = 0.01$. What is the one-sided confidence interval?


## Practice Q1

1. **Test for $H_0: \mu = 0$ vs $H_A: \mu \neq 0$. Take $\alpha = 0.05$.**
```{r}
horse = c(2.0, 1.1, 4.4, -3.1, -1.3, 3.9, 3.2, -1.6, 3.5, 1.2, 2.5, 2.3, 1.9, 1.8, 2.9, -0.3, -2.4)
alpha = 0.05
t.test(horse, alternative = "two.sided", mu = 0, conf.level = 1 - alpha)
```


<span style = "color:blue"> Since p value <$\alpha$, we reject $H_0$ that the population mean $\mu$ is equal to $0$.</span>

## Practice Q2

1. **Confidence interval for $\mu$. Take $\alpha = 0.10$.**

```{r}
horse = c(2.0, 1.1, 4.4, -3.1, -1.3, 3.9, 3.2, -1.6, 3.5, 1.2, 2.5, 2.3, 1.9, 1.8, 2.9, -0.3, -2.4)
alpha = 0.10
t.test(horse, conf.level = 1 - alpha)$conf.int
```

<span style = "color:blue">The $90\%$ CI for $\mu$ is (0.3369827 2.2512526).</span>


## Practice Q3

3. **Test for $\mu \geq 0.5$ at $\alpha = 0.10$. What is the one-sided confidence interval?**

```{r}
horse = c(2.0, 1.1, 4.4, -3.1, -1.3, 3.9, 3.2, -1.6, 3.5, 1.2, 2.5, 2.3, 1.9, 1.8, 2.9, -0.3, -2.4)
alpha = 0.10
t.test(horse, alternative = "greater", mu = 0.5, conf.level = 1 - alpha)
```

* $H_0: \mu < 0.5, H_A: \mu \geq 0.5$.
* Reject since p-value  $< \alpha$.
* CI: $(0.56, -\infty)$.
* <span style="color:blue">Also, reject since CI does not contain 0.5</span>



## Practice: Question 4

4. **Test for $\mu \leq 2$ at $\alpha = 0.01$. What is the one-sided confidence interval?**


```{r}
horse = c(2.0, 1.1, 4.4, -3.1, -1.3, 3.9, 3.2, -1.6, 3.5, 1.2, 2.5, 2.3, 1.9, 1.8, 2.9, -0.3, -2.4)
alpha = 0.01
t.test(horse, alternative = "less", mu = 2, conf.level = 1 - alpha)
```

* $H_0: \mu > 2, H_A: \mu \leq 2$.
* Can't reject since p-value  $> \alpha$.
* CI: $(-\infty, 2.7)$.
* <span style="color:blue">Also, can't reject since CI contains 2.</span>

## Two Sample Test

\newcommand{\X}{\mathrm{X}}


- We learnt hypothesis testing for population mean of **one sample**.
- We used normality assumptions and considered cases for known and unknown (usual case) variances.
- *Application*: Given a dataset, test <span style="color:blue">$H_0$: $\mu = m$ vs $H_A: \mu \neq m$ or $H_A: \mu \lessgtr m$</span>.
  * Both one and two-tailed (sided) tests.
- Reject if <span style="color:blue">$p$ value < $\alpha$</span> or <span style="color:red">confidence interval does not contain the hypothised mean.</span>
- Time to extend  to <span style="color:green">two</span> samples.

## One-Sample $t$ Test

- Suppose you believe that UWF students, on average, rate Taylor Swift's songs at least at $80\%$. You collect a random sample of $100$ students to test your hypothesis.
  * One-sample $t$ test. 
  * <span style="color:blue">One tailed since our interest is $\geq 80\%$ or not.</span>
  * $H_0$: $\mu < 0.80$ vs  $\mu \geq 0.80$ 
  * Test statistic: $T = \frac{\sqrt{n}(\bar{x} - 0.80)}{s}$.
  * Critical value: $c = t_{\alpha, n-1}$.
  * Reject if $T \geq c$.



## Two-Sample $t$ Test: Example 1

- Suppose you believe that UWF students, on average, rate Taylor Swift's songs better than Adele's songs. You ask the **SAME** sample of $100$ students to also provide a rating for Adele's song to test your hypothesis.
  * Two-sample $t$ test.
    * $H_0$: $\mu_{\text{TS}} = \mu_{\text{Adele}}$ vs  $\mu_{{\text{TS}}} \geq \mu_{\text{Adele}}$.
    * Altermatively, $H_0$: $\mu_{\text{TS}} - \mu_{\text{Adele}} = 0$ vs  $\mu_{{\text{TS}}} - \mu_{\text{Adele}} \geq 0$.
  * *Note*: One subject/individual corresponds to one pair of data (Rating of TS, Rating of Adele), i.e., data can be paired by a unique ID or subject or individual.
  
  




## Two-Sample $t$ Test: Example 2

- Suppose you believe that Taylor Swift's songs, on average, are rated better by the students than the faculty. You collected another random sample of $50$ faculties' data to test your hypothesis. 
  * Two-sample $t$ test.
  * $H_0$: $\mu_{\text{student}} = \mu_{\text{faculty}}$ vs  $\mu_{\text{student}} \geq \mu_{\text{faculty}}$ 
  * Rejection: ?
  * Note: Data can not be grouped here by subject.
  
  
  
## Independent Samples

* Suppose we are now comparing two independent groups. 
* Group 1: $\X_1: \{x_{11}, x_{12}, \cdots, x_{1m} \}$. 
* Group 1: $\X_2: \{x_{21}, x_{22}, \cdots, x_{2n} \}$.
* Assumptions: 

  1. $\X_1 \sim \mathbb{N}(\mu_1, \sigma_1^2)$.
  2. $\X_2 \sim \mathbb{N}(\mu_2, \sigma_2^2)$.

* Test: $H_0$: $\mu_1 - \mu_2 = d$ vs  $H_a$: $\mu_1 - \mu_2$ <span style="color:red">$\leq$, $\geq$, or $\neq$</span> d.

##  Case 1: Equal Variance


* **Assumption 3:**   $\sigma_1^2 = \sigma_2^2$.
* **Sample statistics**:
  - $\bar{x}_1$, $\bar{x}_2$: Mean of  $\X_1$ and $\X_2$, respectively.
  - $s_1^2$ and $s_2^2$: Sample variances of  $\X_1$ and $\X_2$, respectively.
* <span style="color:blue;">Under the equal variance assumption, common estimate of variance:
     $$s^2  = \frac{(m-1)s_1^2 \,+ \,(n-1)s_2^2 }{m + n - 2}.$$ </span>
* **Test statistic:** T = $\frac{(\bar{x}_1 - \bar{x}_2) - d}{\sqrt{\frac{s^2}{m} \,+ \, \frac{s^2}{n}}}$.
* **Degrees of freedom:** $\nu = m + n - 2$.



## Case 2: Unequal variance

* **Sample statistics**:
  - $\bar{x}_1$, $\bar{x}_2$: Mean of  $\X_1$ and $\X_2$.
  - $s_1^2$ and $s_2^2$: Variance of  $\X_1$ and $\X_2$.
* **Test statistic:** T = $\frac{(\bar{x}_1 - \bar{x}_2) - d}{\sqrt{\frac{s_1^2}{m} + \frac{s_2^2}{n}}}$
* **Degrees of freedom:** $\nu = \frac{\Big(\frac{s_1^2}{m} + \frac{s_2^2}{n}\Big)^2}{\frac{s_1^2}{m^2(m-1)} + \frac{s_2^2}{n^2(n-1)}}$


## Rejection Criterion

<table style="width:100%; border-collapse: collapse;">
  <tr style="background-color:#f2f2f2;">
    <th style="padding: 8px; border: 1px solid #ddd;"><strong>Alternate Hypothesis</strong></th>
    <th style="padding: 8px; border: 1px solid #ddd;"><strong>Rejection Criterion</strong></th>
  </tr>
  <tr>
    <td style="padding: 8px; border: 1px solid #ddd;">\( H_A: \mu_1 - \mu_2 \leq d \)</td>
    <td style="padding: 8px; border: 1px solid #ddd;">Reject \( H_0 \) if \( T \leq -t_{\alpha; \nu} \)</td>
  </tr>
  <tr>
    <td style="padding: 8px; border: 1px solid #ddd;">\( H_A: \mu_1 - \mu_2 \geq d \)</td>
    <td style="padding: 8px; border: 1px solid #ddd;">Reject \( H_0 \) if \( T \geq t_{\alpha; \nu} \)</td>
  </tr>
  <tr>
    <td style="padding: 8px; border: 1px solid #ddd;">\( H_A: \mu_1 - \mu_2 \neq d \)</td>
    <td style="padding: 8px; border: 1px solid #ddd;">Reject \( H_0 \) if \( T \geq t_{\alpha/2; \nu} \text{ or } T \leq -t_{\alpha/2; \nu} \)</td>
  </tr>
</table>




## Paired $t$-Test

* Data:
    
  <table>
  <tr style="color: blue;">
    <th style="text-align:left;">Subjects</th>
    <th style="text-align:left;">$X_1$</th>
    <th style="text-align:left;">$X_2$</th>
  </tr>
  <tr style="color: blue;">
    <td>1</td>
    <td>$x_{11}$</td>
    <td>$x_{21}$</td>
  </tr>
  <tr style="color: blue;">
    <td>2</td>
    <td>$x_{12}$</td>
    <td>$x_{22}$</td>
  </tr>
  <tr style="color: blue;">
    <td>$\vdots$</td>
    <td>$\vdots$</td>
    <td>$\vdots$</td>
  </tr>
  <tr style="color: blue;">
    <td>n</td>
    <td>$x_{1n}$</td>
    <td>$x_{2n}$</td>
  </tr>
</table>


* **Sample statistics**:
  - *$\bar{x}_1$ and $\bar{x}_2$:* Mean of  $\X_1$ and $\X_2$.
  - *$s^2$*: Variance of  $\X_1 -\X_2$.
* **Test statistic:** T = $\frac{\sqrt{n}(\bar{x}_1 - \bar{x}_2) - d}{s}$
* **Degrees of freedom:** $\nu = n-1$.



## Rejection Criterion: 


<table style="width:100%; border-collapse: collapse;">
  <tr style="background-color:#f2f2f2;">
    <th style="padding: 8px; border: 1px solid #ddd;"><strong>Alternate Hypothesis</strong></th>
    <th style="padding: 8px; border: 1px solid #ddd;"><strong>Rejection Criterion</strong></th>
  </tr>
  <tr>
    <td style="padding: 8px; border: 1px solid #ddd;">\( H_A: \mu_1 - \mu_2 \leq d \)</td>
    <td style="padding: 8px; border: 1px solid #ddd;">Reject \( H_0 \) if \( T \leq -t_{\alpha; \nu} \)</td>
  </tr>
  <tr>
    <td style="padding: 8px; border: 1px solid #ddd;">\( H_A: \mu_1 - \mu_2 \geq d \)</td>
    <td style="padding: 8px; border: 1px solid #ddd;">Reject \( H_0 \) if \( T \geq t_{\alpha; \nu} \)</td>
  </tr>
  <tr>
    <td style="padding: 8px; border: 1px solid #ddd;">\( H_A: \mu_1 - \mu_2 \neq d \)</td>
    <td style="padding: 8px; border: 1px solid #ddd;">Reject \( H_0 \) if \( T \geq t_{\alpha/2; \nu} \text{ or } T \leq -t_{\alpha/2; \nu} \)</td>
  </tr>
</table>




## Example

* I want to test if the hindleg length and foreleg length of deers are the same from the following sample:

```{r echo=F}
# Load necessary library
library(knitr)
library(kableExtra)

# Create a sample data frame with foreleg and hindleg lengths for 10 deers
deer_data <- data.frame(
  Deer = 1:10,
  Foreleg_Length = c(
142,
140,
144,
144,
142,
146,
149,
150,
142,
148),
  Hindleg_Length = c(
138,
136,
147,
139,
143,
141,
143,
145,
136,
146
  )
)

# Generate the kable table
kable(deer_data, col.names = c("Deer", "Foreleg Length (cm)", "Hindleg Length (cm)")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  row_spec(0, bold = TRUE)
```

* Q1: Paired or independent sample? Why?
* One/two tailed?
* Hypotheses?


## Example 1 (cont.)

* Q1: Paired or independent sample? Why?
  - <span style = "color:blue;">Paired, since the pair of the data (Foreleg Length, Hindleg Length) corresponds to a unique subject/deer.
* One/two tailed?
  -  <span style = "color:blue;">Two-tailed</span>.
* Hypotheses?
  - <span style = "color:blue;">Assume $\mu_1$ = Mean foreleg length, $\mu_2$ = $\ldots$</span> <span style = "color:red;">(fill up the assumptions).</span>  
  - <span style = "color:blue;">$H_0: \mu_1 = \mu_2$ vs $H_A: \mu_1 \neq \mu_2$</span>
  
  
## Example 1 (cont.)

```{r echo=F, eval=F}
diff = deer_data$Foreleg_Length - deer_data$Hindleg_Length
mean(diff)
var(diff)
t.test(diff, mu = 0, alternative = "two.sided", conf.level = 0.95)
```



```{r}
foreleg = deer_data$Foreleg_Length
hindleg = deer_data$Hindleg_Length
t.test(foreleg, hindleg, mu = 0, paired = TRUE, alternative = "two.sided", conf.level = 0.95)
```


## Example 1 (cont.)


* **Decision:** 
  
  1. Since p-value < 0.05, we reject $H_0$.
  2. Alternatively, since the $95\%$ CI does not contain $0$, we reject $H_0$.
  
* There is enough evidence that the average difference between leg lengths is not $0$.


## Example 1: Calculation by hand

* Statistics:
  
  1. N = 10, $\bar{X}_1$ (Mean of Foreleg): 144.7, $\bar{X}_2$ (Mean Hindleg): 141.4.
  2. Variance of the difference: $s^2 = 9.34$.
  3. $T =\frac{\sqrt{n}(\bar{X}_1 - \bar{X}_2) - d}{s}$ = $\frac{\sqrt{10}(144.7 - 141.4) - 0}{\sqrt{9.34}}$ = 3.413.
  4. Critical value: c = $t_{\alpha/2, n-1} = t_{0.025, 10-1} = 2.26$.
  5. Reject $H_0$ since T > critical value.

## Example 2.

* Consider the same example. I want to test if average foreleg length is at least 1.5 cm **MORE** than average hindleg length.
  
  * Q1: Paired or independent sample? Why?
  * One/two tailed?
  * Hypotheses?


## Example 2 (cont.)

* Consider the same example. I want to test if average foreleg length is at least 1.5 cm **MORE** than average hindleg length.
  
  * Q1: Paired or independent sample? Why?
    - <span style="color:blue"> Paired. </span>
  * One/two tailed?
    - <span style="color:blue"> One tailed. </span>
  * Hypotheses?
    -   <span style="color:blue"> $H_0: \mu_1 - \mu_2 \leq 1.5$ vs $H_A: \mu_1 - \mu_2 > 1.5$ </span>

## Example 2 (cont)

```{r}
foreleg = deer_data$Foreleg_Length
hindleg = deer_data$Hindleg_Length
t.test(foreleg, hindleg, mu = 1.5, paired = TRUE, alternative = "greater", conf.level = 0.95)
```

* **Decision:** 
  
  1. Since p-value < 0.05, we reject $H_0$.
  2. The $95\%$ CI does not contain $1.5$, we reject $H_0$.
  
* There is enough evidence that the average difference between leg lengths is greater than $1.5$.




## Example 3: (Independent, Equal Variance)

This table shows an example of heights of plants under two different fertilizers. Test if the new fertilizer is working better, i.e., average height under the new fertilizer is more than the earlier one. <br><br>
<center> Old fertilizer: **<span style = "color:red">48.2, 54.6, 58.3, 47.8, 51.4, 52.0, 55.2, 49.1, 49.9, 52.6</span>**</center>
<center> New fertilizer: **<span style = "color:red">52.3, 57.4, 55.6, 53.2, 61.3, 58.0, 59.8, 54.8</span>** </center>


## Example 3: (cont.)


```{r echo =F}
# Sample data for two groups
group1 <- c(
48.2,
54.6,
58.3,
47.8,
51.4,
52.0,
55.2,
49.1,
49.9,
52.6)
group2 <- c(
52.3,
57.4,
55.6,
53.2,
61.3,
58.0,
59.8,
54.8
)

# Calculating sample statistics
mean1 <- mean(group1)
mean2 <- mean(group2)
var1 <- var(group1)
var2 <- var(group2)
n1 <- length(group1)
n2 <- length(group2)

# Create a data frame for the table
t_test_table <- data.frame(
  Statistic = c("Sample Size", "Mean", "Variance"),
  Group1 = c(round(n1), round(mean1, 2), round(var1, 4)),
  Group2 = c(round(n2), round(mean2, 2), round(var2, 4))
)

# Generate the kable table
library(knitr)
library(kableExtra)

kable(t_test_table, col.names = c("Statistic", "Group 1", "Group 2")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  row_spec(0, bold = TRUE)
```

* $H_0$: $\mu_1 \geq \mu_2$ vs $\mu_1 < \mu_2$.
* Estimate for common variance = $s^2$ = $\frac{(10 - 1) * 11.3588 + (8 - 1) * 9.8857}{10 + 8 - 2} = 10.7143$.
* Test statistic: $T = \frac{(51.91	- 56.55)}{\sqrt{10.7143/10 + 10.7143/8 }} = -2.988$.
* Critical value: $-t_{0.05; 10 + 8 - 2} = -1.75$.
* Reject since $T <$ critical value.



## Example 3: (cont.)


```{r}
height = c(48.2, 54.6, 58.3, 47.8, 51.4, 52.0, 55.2, 49.1, 49.9, 52.6,
           52.3, 57.4, 55.6, 53.2, 61.3, 58.0, 59.8, 54.8)
fertilizer = c(rep("Old", 10), rep("New", 8))
t.test(height ~ fertilizer, conf.level = 0.95, mu=0, var.equal = TRUE, alternative = "less")
```

## Example 3: Cont.

* Note: The $t$ statistic is positive, but in our earlier answer, it was negative.
* This is due to $\texttt{R}$; it groups $\texttt{<var1>} \sim \texttt{<var2>}$ in alphabetical order of $\texttt{<var2>}$.
  - $\texttt{"New"}$ before $\texttt{"Old"}$.
* I suggest you always use a prefix:
  - $\texttt{"1.Old"}$ before $\texttt{"2.New"}$.
  
## Example 3: (cont.)


```{r}
height = c(48.2, 54.6, 58.3, 47.8, 51.4, 52.0, 55.2, 49.1, 49.9, 52.6,
           52.3, 57.4, 55.6, 53.2, 61.3, 58.0, 59.8, 54.8)
fertilizer = c(rep("1.Old", 10), rep("2.New", 8))
t.test(height ~ fertilizer, conf.level = 0.95, mu=0, var.equal = TRUE, alternative = "less")
```

* **Decision:** Reject since p-value < 0.05; or since confidence interval doesn't contain $0$.
* **Interpretation:** Enough evidence that new fertilizer leads to higher height of the plants.



## Example 4: Two-Sample t-Test (Independent, Unequal Variance)

The data are the times for seven cockroach eggs to hatch at one laboratory temperature and for eight eggs to hatch at another temperature. Define $\mu_1$ as the true population mean at average hatching days at $30^{\circ}$C; and $\mu_2$ the same at $10^{\circ}$C. I want to test if $\mu_1 - \mu_2 > -13.5$. Take $\alpha = 0.10$. <br> <br>

<center> At $30^{\circ}$C: **<span style = "color:red">40, 38, 32, 37, 39, 41,35</span>**</center>
<center> At $10^{\circ}$C: **<span style = "color:red">36, 45, 32, 52, 59, 41, 48, 55</span>** </center>


## Example 4: (Cont.)

 
* $H_0$: $\mu_1 - \mu_2 \leq -13.5$ vs $\mu_1 - \mu_2 > -13.5$.
* One sided test. Independent Sample.
* $\bar{X}_1 = 37.43, \bar{X}_2 = 46$. 
* However, <span style = "color:red">$s_1^2 = 9.62, s_2^2 = 87.43$ </span>.
* Note the huge difference in sample variances. It indicates in favor of unequal variance case.


## Example 4: (Cont.)


```{r}
hatch_time = c(40, 38 ,32 ,37 ,39 ,41 ,35,
               36, 45 ,32 ,52 ,59 ,41 ,48 ,55)
temperature = c(rep("1.Temp_30C", 7), rep("2.Temp_10C", 8))
t.test(hatch_time ~ temperature, mu=-13.5, alternative="greater", conf.level = 0.9, var.equal = FALSE)
```

* **Decision**: Reject since p-value < $\alpha$ or $90\%$ CI does not contain $-13.5$.


## Example 4: (Cont.)

* What if we disregard unequal variance?

```{r}
hatch_time = c(40, 38 ,32 ,37 ,39 ,41 ,35,
               36, 45 ,32 ,52 ,59 ,41 ,48 ,55)
temperature = c(rep("1.Temp_30C", 7), rep("2.Temp_10C", 8))
t.test(hatch_time ~ temperature, mu=-13.5, alternative="greater", conf.level = 0.9, var.equal = TRUE)
```

* **Decision**: Can't reject since p-value > $\alpha$ or CI contains $-13.5$.






## Overall Instruction

* In  $\texttt{R}$, use $\texttt{t.test(<vector>, alternative = <alt>, mu = <diff>, conf.level = <1-alpha>)}$.
  - <span style="color:blue"> \<alt\> is "two.sided", "less", or "greater".</span>
  - \< diff \> = Hypothised value of $\mu_1 - \mu_2$.</span>
* In case of paired $t$ test, also add the option $\texttt{paired = TRUE}$.
  - <span style="color:blue">By default, $\texttt{paired = FALSE}$ </span>
* In case of independent $t$ test with equal variance, also add the option $\texttt{var.equal = TRUE}$.
  - <span style="color:blue">$\texttt{var.equal = FALSE}$</span>
* <span style="color:red">Try $\texttt{?t.test}$ in $\texttt{R}$ console and read the options.</span>



