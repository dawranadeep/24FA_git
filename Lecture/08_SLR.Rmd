---
title: "Simple Linear Regression"
subtitle: "Part 2"
author: "Ranadeep Daw"
date: "`r Sys.Date()`"
output:
  revealjs::revealjs_presentation:
    theme: "solar"
    center: true
    widescreen: true
    self_contained: true
    width: 1600
    height: 900
    df_print: kable
    reveal_options:
      slideNumber: 'c/t'
      previewLinks: true
---
  
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

## Simple Linear Regression (SLR Model)

1. **Problem Statement**: We have $n$ pair of observations - independent variables/ covariates $\{x_1, x_2 , \ldots, x_n\}$ and dependent variables/ response variables $\{y_1, y_2 , \ldots, y_n\}$.

2. **Model**: We want to <span style="color:blue">model</span> $Y$ as a linear function of $x$ as in:

$$ y_i = \beta_0 + x_i \beta_1  + \epsilon_i $$



## Typical SLR Plot

```{r echo=F}

# Set seed for reproducibility
set.seed(123)

# Generate data for x
x <- mtcars$wt #seq(1, 20, by = 1)

# Generate corresponding y values with a linear relationship and added noise (error)
y <- mtcars$mpg #2 * x + rnorm(length(x), mean = 0, sd = 5)

# Calculate the fitted values from the linear regression model
model <- lm(y ~ x)
fitted_values <- model$fitted.values

# Calculate errors (residuals)
errors <- y - fitted_values

# Plot scatter plot with linear relationship and errors
plot(x, y, main = "Scatter Plot with Linear Relationship and Error Bars",
     xlab = "Weight", ylab = "MPG", pch = 19, col = "black", ylim = c(min(y, fitted_values), max(y, fitted_values)   )  )

# Add the linear regression line
abline(model, col = "deeppink", lwd = 2)

# Add vertical error bars to represent the residuals
iii = errors < 0
arrows(x[iii], fitted_values[iii], x[iii], y[iii], angle = 90, code = 3, length = 0.1, col = "chartreuse1")


# Add vertical error bars to represent the residuals
iii = errors > 0
arrows(x[iii], fitted_values[iii], x[iii], y[iii], angle = 90, code = 3, length = 0.1, col = "blue")


legend("topright", legend = c(expression(paste("y = ", beta[0], " + ", beta[1], "x")), "Negative errors", "Positive errors"),
       col = c("deeppink", "blue", "chartreuse1"), lwd = 2, pch = c(NA, NA, NA), bty = "n", lty = 1, seg.len = 2)

# Add grid for better visualization
grid()



```



## Example problem


```{r echo=F, eval=F}
# Install the kableExtra package if you haven't already
# install.packages("kableExtra")

# Load necessary libraries
library(knitr)
library(kableExtra)

# Create the data frame (using the modified numbers from the previous example)
mortgage_data <- data.frame(
  Year = c(2009:2024),
  `Interest Rate (%)` = c(10.20, 10.10, 9.90, 9.10, 8.30, 7.20, 8.20, 7.80, 7.50, 7.50, 6.80, 7.30, 8.00, 6.90, 6.40, 5.70),
  `Median Home Price` = c(185000, 184500, 176000, 174000, 173500, 174000, 173500, 170500, 175000, 178000, 188500, 204500, 232000, 260000, 310500, 331000)
)

# Create the kable table with scroll box
kable(mortgage_data, format = "html", col.names = c("Year", "30-year Mortgage Interest Rate (%)", "Median Home Price")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE) %>%
  scroll_box(width = "100%", height = "300px")  # This adds a scrollable slider with a set height

```


```{r echo=F}
library(knitr)

Income <- c(35, 45, 55, 65, 75)
Expenditure <- c(5.2, 6.0, 7.1, 7.8, 8.5)

data <- data.frame(
  `Income (in 1000)`= Income,
  `Food Expenditure (in 100)` = Expenditure
)
kable(data, col.names = c("Income (in 1000)", "Food Expenditure(in 100)"))
```

* Using the above data, I want to predict $\texttt{Expenditure}$ ($Y$ here) using  $\texttt{Income}$ as $x$ (or the dependent variable).




## SLR Assumptions

* $\epsilon_i$-s are called the **error terms** or **residuals**.
* **Modeling assumptions**: We make the following assumptions on $\epsilon_i$ :
<br>
  * $\mathbb{E}(\epsilon_i) = 0$;
  * var$(\epsilon_i) = \sigma^2$; <span style="color:red"> (Homoscadasticity assumption) </span> 

## SLR: Least square method


* How many parameters do we have?
  - $\beta_0$, $\beta_1$, and $\sigma^2$.
* We <span style="color:limegreen">estimate</span> the parameters using the **least square method**.
* In least square, we do the following:
    
    - Calculate the sum of squares of errors.
    - Try to minimize it.
    
## Least square (cont.)

\begin{align*}
y_i &= \beta_0 + x_i \beta_1 + \epsilon_i \\
\Rightarrow \epsilon_i &= y_i - \beta_0 - x_i \beta_1 \\
\Rightarrow \epsilon_i^2 &= (y_i - \beta_0 - x_i \beta_1)^2 \\
\Rightarrow \sum_{i=1}^n \epsilon_i^2 &= \sum_{i=1}^n  (y_i - \beta_0 - x_i \beta_1)^2 \\
\end{align*}


* **Goal**: Find values of $\beta_0$, $\beta_1$, and $\sigma^2$ that  minimizes the last expression. Denote these values as $\widehat{\beta}_0$, $\widehat{\beta}_1$, and $\widehat{\sigma}^2$.



## Normal Equations


* To minimize $\sum_{i=1}^n \epsilon_i^2$, we take derivatives of the expression with respect to $\widehat{\beta}_0$ and $\widehat{\beta}_1$ and equal them to $0$. This leads to the following two equations, known as the <span style="color:blue">normal equations:



\begin{align*}
\text{1.} \qquad \Big(\sum_{i=1}^n y_i \Big) &=  n \widehat{\beta}_0 + \Big(\sum_{i=1}^n x_i\Big) \widehat{\beta}_1.\\
\text{2.} \qquad \Big( \sum_{i=1}^n x_i y_i \Big) &=  \Big(\sum_{i=1}^n x_i \Big) \widehat{\beta}_0 + \Big(\sum_{i=1}^n x_i^2 \Big) \widehat{\beta}_1.\\
\end{align*}

* Note: from the data, you can calculate $\sum_{i=1}^n y_i$, $\sum_{i=1}^n x_i$,  $\sum_{i=1}^n x_i y_i$, and $\sum_{i=1}^n x_i^2$.




## Solving the normal equation


* Define: 
  \begin{align*}
  S_{xy} &= \sum_{1=1}^n (x_i - \bar{x}) (y_i - \bar{y}) \\
  &= \sum_{1=1}^n x_i y_i -  \frac{\Big(\sum_{1=1}^n x_i \Big) \Big(\sum_{1=1}^n y_i \Big)}{n} \\
  S_{xx} &= \sum_{1=1}^n (x_i - \bar{x})^2 \\
  &= \sum_{1=1}^n x_i^2 - \frac{\Big(\sum_{1=1}^n x_i \Big)^2}{n}.
  \end{align*}
  



## Solving (Contd.)

* Then you can solve for  $\widehat{\beta}_0$ and $\widehat{\beta}_1$ as below:


\begin{align*}
\widehat{\beta}_1 &= \frac{S_{xy}}{S_{xx}}. \\
\widehat{\beta}_0 &= \frac{1}{n} \big( \sum_{i=1}^n y_i \big) - \frac{1}{n} \big( \sum_{i=1}^n x_i \big)\widehat{\beta}_1 \\
&= \bar{y} - \bar{x} \widehat{\beta}_1.\\
\end{align*}


## Fitted SLR

* We <span style="color:maroon">estimated</span> the two unknown parameters in the regression line.

* Our estimated <span style="color:maroon">sample regression line</span> is:
  
  \begin{align*}
  \widehat{y}_i = \widehat{\beta}_0 + x_i \widehat{\beta}_1
  \end{align*}

* $\widehat{y}_i$ is our predicted value from the model.
* $e_i = y_i- \widehat{y}_i$ is the fitted error or residual part that we can not explain.


## Example (Contd.)


```{r echo=F}
# Load necessary libraries
library(knitr)
library(kableExtra)

Income <- c(35, 45, 55, 65, 75)
Expenditure <- c(5.2, 6.0, 7.1, 7.8, 8.5)
# Create a data frame with the values
data <- data.frame(
  `Income (in 1000)`= Income,
  `Food Expenditure (in 1000)` = Expenditure,
  `Income^2` = Income^2,
  `Expenditure^2` = Expenditure^2,
  `Income * Expenditure` = Income * Expenditure
)

# Calculate the sums
sum_xi <- sum(data[,1])
sum_yi <- sum(data[,2])
sum_xi_sq <- sum(data[,3])
sum_yi_sq <- sum(data[,4])
sum_xi_yi <- sum(data[,5])

# Add a row of sums to the data frame
data <- rbind(data, c(sum_xi, sum_yi, sum_xi_sq, sum_yi_sq, sum_xi_yi))


# Create the kable table
kable(data, format = "html", escape = FALSE,
      col.names = c("$x_i$", "$y_i$", "$x_i^2$", "$y_i^2$", "$x_i y_i$")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  row_spec(nrow(data), bold = TRUE) %>%  # Bold the last row
  row_spec(nrow(data) - 1, hline_after = TRUE) %>%
  footnote(general = "The last row contains the sums for each column.") # Add a double line before the last row
```

## Cont.

* Here, $n = 5$.
<br>
* $\sum_{i=1}^n x_i^2 = 16125; \sum_{i=1}^n x_i = 275$.
* $S_{xx} = 16125 - 275^2/5 = 1000$.
<br>
* $\sum_{i=1}^n x_i y_i = 1987; \sum_{i=1}^n x_i = 275; \sum_{i=1}^n y_i = 34.6$.
* $S_{xy} = 1987 - 275* 34.6/5 = 84$.

## Cont.

* $\widehat{\beta}_1 = \frac{S_{xy}}{S_{xx}} = \frac{84}{1000} = 0.084$.
* $\widehat{\beta}_0 = 34.6/5 - 275/5 * 0.084 = 2.3$.

<br>


* Hence, our fitted regression line is:

<span style="color:red"> $$\widehat{y}_i = 2.3  + 0.084 x_i. $$ </span>



## Plot


```{r echo=F}

# Set seed for reproducibility
set.seed(123)

# Generate data for x
x <- Income

# Generate corresponding y values with a linear relationship and added noise (error)
y <- Expenditure

# Calculate the fitted values from the linear regression model
model <- lm(y ~ x)
fitted_values <- model$fitted.values

# Calculate errors (residuals)
errors <- y - fitted_values

# Plot scatter plot with linear relationship and errors
plot(x, y, main = "Scatter Plot for Income (x) vs Expenditure (y)",
     xlab = "Income", ylab = "Expenditure", pch = 19, col = "black", ylim = c(min(y, fitted_values), max(y, fitted_values)   )  )

# Add the linear regression line
abline(model, col = "deeppink", lwd = 2)

# Add vertical error bars to represent the residuals
iii = errors < 0
arrows(x[iii], fitted_values[iii], x[iii], y[iii], angle = 90, code = 3, length = 0.1, col = "chartreuse1")


# Add vertical error bars to represent the residuals
iii = errors > 0
arrows(x[iii], fitted_values[iii], x[iii], y[iii], angle = 90, code = 3, length = 0.1, col = "blue")


legend("topleft", legend = c(expression(paste("y = ", beta[0], " + ", beta[1], "x")), "Negative errors", "Positive errors"),
       col = c("deeppink", "blue", "chartreuse1"), lwd = 2, pch = c(NA, NA, NA), bty = "n", lty = 1, seg.len = 2)

# Add grid for better visualization
grid()



```



## Interpretation

* $\widehat{\beta}_0$ is the estimated value of the response ($y$) when $x$ is 0.
* $\widehat{\beta}_1$ is the estimated change in the response ($y$) per unit increase in the covariate $x$. 
* If $\widehat{\beta}_1 > 0$, $y$ increases with increasing $x$, i.e., positive relationship.
* If $\widehat{\beta}_1 < 0$, $y$ decreases with increasing $x$, i.e., negative relationship.


## Prediction

* Once I have my fitted line, I can first generate a prediction.

\begin{align*} 
\widehat{y}_1 &= 2.3  + 0.084 * (35) = 5.24 \\
\widehat{y}_2 &= 2.3  + 0.084 * (45) = 6.08 \\
\widehat{y}_3 &= 2.3  + 0.084 * (55) = 6.92 \\
\widehat{y}_4 &= 2.3  + 0.084 * (65) = 7.76 \\
\widehat{y}_5 &= 2.3  + 0.084 * (75) = 8.60 \\
\end{align*}

## Errors/ Residuals

* Once I have my predictions, I can now calculate the prediction errors.


\begin{align*} 
e_1 = y_1 - \widehat{y}_1 &= 5.2 - 5.24 = -0.04\\
e_2 = y_2 - \widehat{y}_2 &= 6.0 - 6.08 = -0.08\\
e_3 = y_3 - \widehat{y}_3 &= 7.1 - 6.92 = 0.18\\
e_4 = y_4 - \widehat{y}_4 &= 7.8 - 7.76 = 0.04\\
e_5 = y_5 - \widehat{y}_5 &= 8.5 - 8.60 = -0.10\\
\end{align*}


## Final table


```{r echo=F}
# Load necessary libraries
library(knitr)
library(kableExtra)

Income <- c(35, 45, 55, 65, 75)
Expenditure <- c(5.2, 6.0, 7.1, 7.8, 8.5)
mod <- lm(Expenditure ~ Income)
Pred <- mod$fitted.values
Res <- mod$residuals
# Create a data frame with the values
data <- data.frame(
  `Income (in 1000)`= Income,
  `Food Expenditure (in 1000)` = Expenditure,
  `Income^2` = Income^2,
  `Expenditure^2` = Expenditure^2,
  `Income * Expenditure` = Income * Expenditure,
  Residuals = Pred,
  Errors = Res
)




# Create the kable table
kable(data, format = "html", escape = FALSE,
      col.names = c("$x_i$", "$y_i$", "$x_i^2$", "$y_i^2$", "$x_i y_i$", "$\\widehat{y}_i$", "$e_i$") ) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) 
```



## True vs Estimated Regression line


|                      | **True Regression Line**                            | **Sample Regression Line**                         |
|----------------------------------|-----------------------------------------------------------|----------------------------------------------------|
| **Equation**                     | \( y_i = \beta_0 + \beta_1 x_i + \epsilon \)                  | \( \hat{y}_i = \widehat{\beta}_0 + x_i \widehat{\beta}_1 \)                        |
| **Intercept Coefficient**                 | \( \beta_0 \)       | \( \widehat{\beta}_0 \)           |
| **Slope Coefficient**                 |  \( \beta_1 \)           | \( \widehat{\beta}_1 \)            |
| **Error term**                   |  \( \epsilon_i \)                 |  \( e_i = y_i - \widehat{y}_i \) |
| **Error variance**| var$(\epsilon_i) = \sigma^2$ |    $\widehat{\sigma}^2$ |



## Error variance estimation


\begin{align*}
\widehat{\sigma}^2 &=\frac{1}{n-2}  \sum_{i=1}^n  e_i^2 \\
&= \frac{1}{n-2} \sum_{i=1}^n  \big(y_i - \widehat{y}_i \big)^2 \\
&= \frac{1}{n-2} \sum_{i=1}^n \big(y_i - \widehat{\beta}_0 - x_i \widehat{\beta}_1 \big)^2 \\
\end{align*}

<br>

* Error standard devitation: $\widehat{\sigma} = \sqrt{\widehat{\sigma}^2}$.





## Example Continued

* Estimated error variance = $\widehat{\sigma}^2 = \frac{1}{5-2} \big( (-0.04)^2 + (−0.08)^2 +  0.18^2 + 0.04^2$ + $(−0.10)^2 \big) = 0.0173$.

* So, estimated error standard deviation = $\sqrt{0.0173} = 0.1317$.



## $\texttt{R}$ Syntax

* Modeling syntax:

```{r eval=F}
<model_name> = lm(<y_column> ~ <x_column>, data = <data_name>)
```

* Seeing the output:

```{r eval=F}
summary(<model_name>)
```


## Example Continued

* Typical question format will look like this:
  
  - The following dataframe, named $\texttt{income_data}$ is a dataset of income (in $\$ 1000$) and food expenditure (in $\$ 100$) of 5 families. Using income as the independent variable ($X$) and food expenditure as the dependent variable $(Y)$, build a simple linear regression model.
  
```{r echo=F}
income_data = data.frame(
  Income = c(35, 45, 55, 65, 75),
  Expenditure = c(5.2, 6.0, 7.1, 7.8, 8.5)
)

kable(income_data)
```
  
  
## Example (cont.)


```{r}
income_data = data.frame(
  Income = c(35, 45, 55, 65, 75),
  Expenditure = c(5.2, 6.0, 7.1, 7.8, 8.5)
)
income_data
```
  
  
## Example (cont.)

```{r}
my_model = lm(Expenditure ~ Income, data = income_data)
summary(my_model)
```


## Summary Output



![](https://raw.githubusercontent.com/dawranadeep/24FA_git/refs/heads/main/Practice_qns/img/slr_output2.png)

* See the above output [here](https://raw.githubusercontent.com/dawranadeep/24FA_git/refs/heads/main/Practice_qns/img/slr_output2.png).



## Summary of SLR Output: Equation

* **Question:** How can you write the estimated regression model from the output?
<br>
  - Look at the <span style="color:red">Coefficients</span> table's <span style="color:red">Estimate</span> column.
  - First one is the intercept ($\widehat{\beta}_0$); second one is the slope ($\widehat{\beta}_1$).
  - So here the estimated regression equation is:
  
  $\widehat{\text{Expenditure}} = 2.3 + 0.084 *$ Income. 


## Summary of SLR Output: Error Standard Deviation

* **Question:** What is the estimated error standard deviation from the output?
  - Look at the <span style="color:green">Residual Standard Error</span>.
  - $\widehat{\sigma} = 0.1317.$

* **Question:** What is the estimated error variance from the output?
  - You have to square the residual standard error.
  - $\widehat{\sigma}^2 = 0.1317^2 = 0.0173.$
  
  
## Link with correlation

* Last class, we discussed correlation:
  $$r = \frac{1}{n-1} \sum_{i=1}^n \Big(\frac{x_i - \bar{x}}{s_X}\Big)  \Big(\frac{y_i - \bar{y}}{s_Y}\Big)$$
* We can link correlation with regression here as below:
  - $\widehat{\beta}_1 = r \sqrt{\frac{S_{yy}}{S_{xx}}}$
  - $S_{yy} = \sum_{1=1}^n (y_i - \bar{y})^2 = \sum_{1=1}^n y_i^2 - \frac{\Big(\sum_{1=1}^n y_i \Big)^2}{n}$.
  
  

## $R$-square ($R^2$)

* To evaluate the strength of the regression model, we define the $R$-square.
* In SLR, this is the square of the correlation coefficient $r$.
* Since $-1 \leq r \leq +1$, $0 \leq R^2 \leq 1$.
* A high $R^2$ value indicates a strong linear relationship between $x$ and $y$. 




## A property regarding $R^2$

* You can show that:
\begin{align*}
\sum_{i=1}^n \big(y_i - \bar{y} \big)^2 &= \sum_{i=1}^n \big(\widehat{y}_i - \bar{y} \big)^2  + \sum_{i=1}^n \big(y_i - \widehat{y}_i \big)^2 
\end{align*}

* The first term is $S_{yy}$ or total variability in the response. It is denoted as SST (total sum of squares).
* The second term is **regression sum of squares**, denoted as SS$_{\text{reg}}$.
* The last term is **error sum of squares**, denoted as SSE.
* SST = SS$_{\text{reg}}$ + SSE.

## $R$-square: Properties and Interpretation

* $R^2 =  \widehat{\beta}_1^2 \frac{S_{xx}}{S_{yy}}$.
* $R^2 =  1 - \frac{S_{ee}}{S_{yy}}$, where S$_{ee}$ is the total residual sum of squares = $\sum_{1=1}^n (e_i - \bar{e})^2 = \sum_{1=1}^n e_i^2$.
* $R^2 = \frac{S_{\text{reg}}}{S_{yy}}$, where $S_{\text{reg}} = S_{yy} - S_{ee}$.
* $R^2$ is the fraction of variability explained of $y$ divided by the total variability in $y$. So, in other words, $R^2$ is the percentage of explained variability of $y$ by the model.

## Summary of SLR Output: $R^2$

* **Question:** What is the $R^2$ of the regression model from the output?
  - Look at the <span style="color:blue">Multiple R-squared</span> option in the output.
  - $R^2 = 0.9927$.
  
## Does the model make sense?

* We can always fit a linear model like this.
* But sometimes there is just no relationship between x and y.
* Hence, it is our goal to check if the model is sensible or not.
* There are two ways to check it -- you will understand the differences better in the multiple linear regression lecture.

## Hypothesis Test 1: Test for the model performance

* First, if there is no relationship, the slope coefficient should have a value close to $0$.
* Hence, we want to test the null hypothesis $H_0:$ The model has no explanatory power, $\beta_1 = 0$ (the slope is zero). 
* Alternate hypothesis is $H_A: \beta_1 \neq 0$, i.e., the slope is not zero.
* **Careful, we are not testing with $\widehat{\beta}_1$.**
* For this, we need an additional assumption that:
  $\epsilon_i \sim \mathbb{N}(0, \sigma^2)$


## Cont.

* Our test statistic is: $F = \frac{S_{ee}/1}{(S_{yy} - S_{ee})/(n-2)}$.
* Under $H_0$, the F-statistic follows a **F** distribution with degrees of freedoms $(1, n-2)$.
* We reject the $H_0$ when $p$-value < $\alpha$.


## SLR Output: Model Performance

* **Question:** From the output, is the regression model explaining anything at $\alpha = 0.05$?
  - Look at the very last <span style = "color:LightPink">$p$-value</span> in the output.
  - Since $p$-value (0.00026) < $\alpha$ (0.05), we reject $H_0$ that the model is meaningless.
  - So, here my model is explaining something about $y$ from $x$. 
  
  


## Hypothesis Test 2: Test for Individual Coefficients

* Another test for $H_0$: $\beta_1 = 0$ vs $H_A$: $\beta_1 \neq 0$.
* This time, we will perform a one sample $t$ test.
* Same hypothesis as before, but this is a different test.
* You will see the differences between the $t$ and the $F$ test more in the multiple linear regression case.


## Cont.

* Recall, the test statistic for the $t$ test has a form:
 $$ T = \frac{\widehat{\beta}_1 - 0}{ \text{sd } (\widehat{\beta}_1 )}. $$
* At level $\alpha$, we reject the $H_0$ if $|T| > t_{\alpha/2; n-1}$.

## SLR Output: Test for $\beta_1 = 0$

* From the $\texttt{R}$ output, look the <span style="color:#FFDB58"> Coefficients table, last column named *Pr(>|t|)* </span>. Look for the row corresponding to $\widehat{\beta}_1$.
* This is the $p$ value. Reject $H_0$ if $p$ value < $\alpha$.
* Also note, in the coefficients table, column $1$ (named *Estimate*) is the $\widehat{\beta}_1$.
* Column $2$ is the sd of $\widehat{\beta}_1$.
* The column $3$ (named *t value*) is the $t$ statistic.


## SLR Output: Test for $\beta_1 = 0$ (cont.)

* **Question:** From the output, can you test if $\beta_1 = 0$ at $\alpha = 0.05$?
  - Look at the <span style = "color:#FFDB58"> column *Pr(>|t|)*  in the coefficients table; row 2 (or the row for $\beta_1$</span>.
  - Since $p$-value (0.00026) < $\alpha$ (0.05), we reject $H_0$ that $\beta_1 = 0$.
  - Also note that here the $t$ statistic is: **20.176** (column 3).
  
  

## Leverage Score

- **Leverage score** quantifies how much influence an individual data point has on the predicted values.
- It has the following formula: 
  \[
  h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^n (x_j - \bar{x})^2}
  \]
  
  
## Leverage Score (cont.)

- Points with **higher leverage** are further from the mean of the predictor variable \( X \), i.e., more extreme observation in \( X \).
- Points with **higher leverage** cause large changes in the parameter estimates ($\widehat{\beta}_0$, $\widehat{\beta}_1$,$\widehat{\sigma}^2$) when they are deleted. 


## $\texttt{R}$ Syntax

```{r eval=F}
plot(hatvalues(<model_name>), type = 'h')
```

* Example: 

```{r eval=T, out.width=400}
plot(hatvalues(my_model), type = 'h')
```


## Interpreting Leverage Score

- Leverage scores range between 0 and 1, i.e., $0 \leq h_i \leq 1$.
- A **high leverage** point is usually defined as one where:
  \[
  h_i > \frac{2}{n}
  \]

- You may see other thresholds too. In practical examples, you have to experiment with different values to check what's working.
- High leverage points may not necessarily be outliers, but they can significantly affect the regression model's fit.




## Cook's Distance

- **Cook's Distance** is a measure used to assess the influence of an observation on the regression model.
- **Cook's Distance** quantifies how much the regression coefficients would change if a particular observation was removed from the model.
- It combines both the **residual** (how far the actual point is from the predicted value) and the **leverage** (how far the point is from the mean of the predictor).



## Formula for Cook's Distance

For the \( i \)-th observation, Cook’s distance \( D_i \) is calculated as:

\[
D_i = (n-2) \frac{e_i^2}{ \sum_{j=1}^n e_j^2} \cdot \frac{h_i}{(1 - h_i)^2}
\]


- \( e_i \) is the **residual** for the \( i \)-th observation.
- \( h_i \) is the **leverage score** for the \( i \)-th observation.


## Interpreting Cook's Distance

* You will see various thresholds for **high influence** such as:
  - $D_i > 1$
  - $D_i > 3 \bar{D}$

* In real data, you have to keep experimenting which one works best for you.



## $\texttt{R}$ Syntax

```{r eval=F}
plot(cooks.distance(<model_name>), type = 'h')
```

* Example: 

```{r eval=T, out.width=400}
plot(cooks.distance(my_model), type = 'h')
```



